{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create data : from sklearn.datasets import make_blobs \n",
    "#https://www.programcreek.com/python/example/100588/keras.layers.normalization.BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "n_samples=100000\n",
    "n_features=20\n",
    "X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=5, n_redundant=2, n_repeated=0, n_classes=2,\\\n",
    "                           n_clusters_per_class=2, weights=[0.9,0.1], flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0,\\\n",
    "                           scale=1.0, shuffle=True, random_state=None)\n",
    "y=np.array(y)\n",
    "X=np.array(X)\n",
    "y=np.reshape(y,(n_samples, 1))\n",
    "X=np.reshape(X, (n_samples, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-540ce05ef526>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-540ce05ef526>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    def construct_model(layers=[50,50], activation='relu'b_init='random_uniform', k_init='random_uniform', dropout=.0,                    a_dropout=.0, optimizer='sgd', loss='binary_crossentropy', metrics='roc_auc_score',input_dim=20, output_dim=1, model_name='most'):\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.noise import AlphaDropout\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.initializers import lecun_uniform\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import regularizers\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import callbacks\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from sklearn import pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def construct_model(layers=[50,50], activation='relu', b_init='random_uniform', k_init='random_uniform', dropout=.0,\\\n",
    "                    a_dropout=.0, optimizer='sgd', loss='binary_crossentropy', metrics='roc_auc_score',input_dim=20, output_dim=1, model_name='most'):\n",
    "    \"\"\"\n",
    "    Helper to construct a Keras model based on dict of specs and input size\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_spec: dict\n",
    "        Dict containing keys: arch, activation, dropout, optimizer, loss,\n",
    "            w_reg, metrics\n",
    "    input_dim: int\n",
    "        Size of input dimension\n",
    "    output_dim: int\n",
    "        Size of input dimension\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: Compiled keras.models.Sequential\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    for li, layer_size in enumerate(layers):\n",
    "        # For input layer, add input dimension\n",
    "        if li == 0:\n",
    "            temp_input_dim = input_dim\n",
    "            model.add(Dense(layer_size,\n",
    "                            input_dim=temp_input_dim,\n",
    "                            activation=activation,\n",
    "                            bias_initializer=b_init,\n",
    "                            kernel_initializer=k_init,\n",
    "                            name='Input'))\n",
    "        elif li < len(layers)-1:\n",
    "            model.add(Dense(layer_size,\n",
    "                            activation=activation,\n",
    "                            bias_initializer=b_init,\n",
    "                            kernel_initializer=k_init,\n",
    "                            name='Layer_%i' % li))\n",
    "        elif li == len(layers)-1:\n",
    "            model.add(Dense( output_dim ,\n",
    "                            activation='sigmoid',\n",
    "                            bias_initializer=b_init,\n",
    "                            kernel_initializer=k_init,\n",
    "                            name='Layer_%i' % li))\n",
    "            \n",
    "        if dropout > 0. :\n",
    "            if a_dropout == 0.:\n",
    "                model.add(Dropout(dropout, name='Dropout_%i' % li))\n",
    "        if a_dropout > 0.:\n",
    "            model.add(Dropout(a_dropout, name='Dropout_%i' % li))\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=[metrics],\n",
    "                 loss_weights=[2])\n",
    "    #keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, save_best_only=True, \\\n",
    "    #                                write_graph=True, write_images=False)\n",
    "\n",
    "    return model \n",
    "#early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=1, verbose=0, mode='auto')\n",
    "pipe = pipeline.Pipeline([\n",
    "    ('rescale', preprocessing.StandardScaler()),\n",
    "    ('nn', KerasClassifier(build_fn=construct_model, epochs=10, batch_size=128,\n",
    "                           validation_split=0.2))#, callbacks=[early_stopping]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_model_no_act(layers=[50,50], activation='relu', a_reg='l1', b_reg='l1', b_init='random_uniform', k_init='random_uniform', dropout=.0,\\\n",
    "                    a_dropout=.0, optimizer='sgd', loss='binary_crossentropy', metrics='accuracy',input_dim=20, output_dim=1, model_name='most'):\n",
    "    \"\"\"\n",
    "    Helper to construct a Keras model based on dict of specs and input size\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_spec: dict\n",
    "        Dict containing keys: arch, activation, dropout, optimizer, loss,\n",
    "            w_reg, metrics\n",
    "    input_dim: int\n",
    "        Size of input dimension\n",
    "    output_dim: int\n",
    "        Size of input dimension\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: Compiled keras.models.Sequential\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    for li, layer_size in enumerate(layers):\n",
    "        # For input layer, add input dimension\n",
    "        if li == 0:\n",
    "            temp_input_dim = input_dim\n",
    "            model.add(Dense(layer_size,\n",
    "                            input_dim=temp_input_dim,\n",
    "                            activation=activation,\n",
    "                            bias_initializer=b_init,\n",
    "                            kernel_initializer=k_init,\n",
    "                            name='Input'))\n",
    "        elif li < len(layers)-1:\n",
    "            model.add(Dense(layer_size,\n",
    "                            activation=activation,\n",
    "                            bias_initializer=b_init,\n",
    "                            kernel_initializer=k_init,\n",
    "                            name='Layer_%i' % li))\n",
    "        elif li == len(layers)-1:\n",
    "            model.add(Dense( output_dim ,\n",
    "                            activation=None,\n",
    "                            bias_initializer=b_init,\n",
    "                            kernel_initializer=k_init,\n",
    "                            name='Layer_%i' % li))\n",
    "            \n",
    "        if dropout > 0.:\n",
    "            model.add(Dropout(dropout, name='Dropout_%i' % li))\n",
    "        if a_dropout > 0.:\n",
    "            model.add(Dropout(a_dropout, name='Dropout_%i' % li))\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=[metrics])\n",
    "    #keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, save_best_only=True, \\\n",
    "    #                                write_graph=True, write_images=False)\n",
    "    return model \n",
    "#early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=1, verbose=0, mode='auto')\n",
    "pipe2 = pipeline.Pipeline([\n",
    "    ('rescale', preprocessing.StandardScaler()),\n",
    "    ('nn', KerasClassifier(build_fn=construct_model_no_act, epochs=10, batch_size=128,\n",
    "                           validation_split=0.2))#, callbacks=[early_stopping])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#custom scoring function sklearn\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def lift_index(y, probabilities):\n",
    "    #rearrange probabilities and y in descending order\n",
    "    probabilities = [val[0] for val in probabilities]\n",
    "    print(probabilities)\n",
    "    combine = list(zip(y, probabilities))\n",
    "    dtype = [('data', int), ('prob', float)]\n",
    "    combine = np.array(combine,dtype=dtype)\n",
    "    combine = np.sort(combine, order='prob') #sort according to probabilities\n",
    "    ordered = np.sort(probabilities) #sort probabilities\n",
    "    #divide into deciles\n",
    "    deciles = pd.qcut(ordered, 10, duplicates='drop', labels=False) #creates array of digits from 1 to 10\n",
    "    #sum of positive in each decile-if negative then zero and doesn't get taken into account\n",
    "    sum_deciles = sum([((val+1)/10)*combine[num][0] for num, val in enumerate(deciles)])\n",
    "    return sum_deciles/sum(y)\n",
    "lift_index_score = make_scorer(lift_index, greater_is_better=True, needs_proba=True )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 1, 1, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6, 7, 7, 8],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=[0,0,0,0,0,0,1,0,0,0,1,0,1,0,1,0,1,0,1,1]\n",
    "probabilities = [0.6, 0.7,0.8,0.1,0.2,0.4,0.6,0.7,0.8,0.03,0.2, 0.4,0.2,0.9,0.2,0.75,0.243,0.5,0.75,0.03]\n",
    "combine = list(zip(y, probabilities))\n",
    "dtype = [('data', int), ('prob', float)]\n",
    "combine = np.array(combine,dtype=dtype)\n",
    "combine = np.sort(combine, order='prob') #sort according to probabilities\n",
    "ordered = np.sort(probabilities) #sort probabilities\n",
    "#divide into deciles\n",
    "deciles = pd.qcut(ordered, 10, duplicates='drop',labels=False) #creates array of digits from 1 to 10\n",
    "sum_deciles = sum([((val+1)/10)*combine[num][0] for num, val in enumerate(deciles)])\n",
    "deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title='Normalized confusion matrix'\n",
    "    else:\n",
    "        title='Confusion matrix'\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_many_score(results, scoring):\n",
    "    \"\"\"\n",
    "    This function prints and plots a graph conditioned on some variable that shows progress in accuracy.\n",
    "    results is the cv_results_ attribute after calling gs callable.\n",
    "    scoring is dict of scoring functions.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(13, 13))\n",
    "    plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",\n",
    "              fontsize=16)\n",
    "    \n",
    "    plt.xlabel(\"min_samples_split\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid()\n",
    "    \n",
    "    ax = plt.axes()\n",
    "    ax.set_xlim(0, 402)\n",
    "    ax.set_ylim(0.73, 1)\n",
    "    \n",
    "    # Get the regular numpy array from the MaskedArray\n",
    "    X_axis = np.array(results['param_min_samples_split'].data, dtype=float)\n",
    "    \n",
    "    for scorer, color in zip(sorted(scoring), ['g', 'k']):\n",
    "        for sample, style in (('train', '--'), ('test', '-')):\n",
    "            sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n",
    "            sample_score_std = results['std_%s_%s' % (sample, scorer)]\n",
    "            ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "                            sample_score_mean + sample_score_std,\n",
    "                            alpha=0.1 if sample == 'test' else 0, color=color)\n",
    "            ax.plot(X_axis, sample_score_mean, style, color=color,\n",
    "                    alpha=1 if sample == 'test' else 0.7,\n",
    "                    label=\"%s (%s)\" % (scorer, sample))\n",
    "    \n",
    "        best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "        best_score = results['mean_test_%s' % scorer][best_index]\n",
    "    \n",
    "        # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "        ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
    "                linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
    "    \n",
    "        # Annotate the best score for that scorer\n",
    "        ax.annotate(\"%0.2f\" % best_score,\n",
    "                    (X_axis[best_index], best_score + 0.005))\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <keras.wrappers.scikit_learn.KerasClassifier object at 0x0000016FB0236F60>, as the constructor does not seem to set parameter callbacks",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ba52543c18f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m                                \u001b[0mrefit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                scoring= 'roc_auc')\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#reg search\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mgrid_result_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[1;31m#reg search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Snir\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    623\u001b[0m                                      n_candidates * n_splits))\n\u001b[1;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mbase_estimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0mpre_dispatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Snir\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mnew_object_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mnew_object_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mnew_object\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mparams_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Snir\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[1;31m# XXX: not handling dictionaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mestimator_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mestimator_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'get_params'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Snir\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[1;31m# XXX: not handling dictionaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mestimator_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mestimator_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'get_params'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Snir\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[1;31m# XXX: not handling dictionaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mestimator_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mestimator_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'get_params'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Snir\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[1;31m# XXX: not handling dictionaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mestimator_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mestimator_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'get_params'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Snir\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m    117\u001b[0m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[1;32m    118\u001b[0m                                \u001b[1;34m'does not seem to set parameter %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                                (estimator, name))\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot clone object <keras.wrappers.scikit_learn.KerasClassifier object at 0x0000016FB0236F60>, as the constructor does not seem to set parameter callbacks"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "#fix seed for reproducibility\n",
    "seed=7\n",
    "np.random.seed(seed)\n",
    "n_samples=100000\n",
    "n_features=20\n",
    "X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=5, n_redundant=2, n_repeated=0, n_classes=2,\\\n",
    "                           n_clusters_per_class=2, weights=[0.9,0.1], flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0,\\\n",
    "                           scale=1.0, shuffle=True, random_state=None)\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "#import data without any dummy variables or changes\n",
    "param_grid = {'nn__layers': [[100,100],[100,100,100],[50,50,50,50]],\n",
    "              'nn__activation':['selu'],\n",
    "              'nn__dropout': [0.],\n",
    "              'nn__a_dropout': [0.2],\n",
    "              'nn__k_init': ['lecun_normal'],\n",
    "              'nn__b_init': ['lecun_normal'],\n",
    "              'nn__optimizer':['adam'],\n",
    "              'nn__metrics' : ['accuracy'],\n",
    "              'nn__epochs':[10],\n",
    "              'nn__batch_size':[32]}\n",
    "\n",
    "#improve with article\n",
    "grid = GridSearchCV(pipe,\n",
    "                    param_grid,\n",
    "                    return_train_score=True,\n",
    "                    refit=True,\n",
    "                    scoring='roc_auc',\n",
    "                    verbose=15)\n",
    "rand_param_grid = {'nn__layers': [[100,100],[100,100,100],[50,50,50,50]],\n",
    "              'nn__activation':['relu','selu'],\n",
    "              'nn__dropout': scipy.stats.expon(scale=0.5),\n",
    "              'nn__a_dropout': scipy.stats.expon(scale=0.2),\n",
    "              'nn__k_init': ['lecun_normal'],\n",
    "              'nn__b_init': ['lecun_normal'],\n",
    "              'nn__optimizer':('rmsprop','adam'),\n",
    "              'nn__metrics' : ['accuracy'],\n",
    "              'nn__epochs':[10,50],\n",
    "              'nn__batch_size':[32,16]} \n",
    "#improve with article\n",
    "rand_grid = RandomizedSearchCV(pipe2, \n",
    "                               rand_param_grid,\n",
    "                               return_train_score=True,\n",
    "                               refit=True,\n",
    "                               scoring= 'roc_auc')\n",
    "grid_result = grid.fit(X_train,y_train) #reg search\n",
    "grid_result_2 = rand_grid.fit(X_train,y_train)\n",
    "#reg search\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "#random searh\n",
    "means2 = grid_result_2.cv_results_['mean_test_score']\n",
    "stds2 = grid_result_2.cv_results_['std_test_score']\n",
    "params2 = grid_result_2.cv_results_['params']\n",
    "for mean, stdev, param in zip(means2, stds2, params2):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualize best model\n",
    "n_classes = 2\n",
    "model_name = 'dense'\n",
    "plot_confusion_matrix(confusion_matrix(grid_result.predict_classes(X_test), y_test), n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=grid_result #redundant variable change \n",
    "best = construct_model( layers=model.best_params_['layers'],\n",
    "              activation=model.best_params_['activation'],\n",
    "              b_init=model.best_params_['b_init'],\n",
    "              k_init=model.best_params_['k_init'],\n",
    "              dropout=model.best_params_['dropout'],\n",
    "              a_dropout=model.best_params_['a_dropout'],\n",
    "              optimizer=model.best_params_['optimizer'],\n",
    "              metrics=model.best_params_['metrics'] )\n",
    "plot_history(best.fit(X_train, y_train, validation_split=0.33, epochs=50, batch_size=10, verbose=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "def save(model, model_name):\n",
    "    model_path = \"saved_model/%s.json\" % model_name\n",
    "    weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "    options = {\"file_arch\": model_path,\n",
    "                \"file_weight\": weights_path}\n",
    "    json_string = model.to_json()\n",
    "    open(options['file_arch'], 'w').write(json_string)\n",
    "    model.save_weights(options['file_weight'])\n",
    "\n",
    "save(best, \"best model everrr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
